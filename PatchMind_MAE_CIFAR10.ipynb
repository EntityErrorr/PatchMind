{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "#Masked Patch Prediction Using Vision Transformers on CIFAR-10: A Self-Supervised Learning Approach\n",
        "\n",
        "This notebook implements a simplified version of Masked Autoencoder (MAE) using Vision Transformers (ViT) on the CIFAR-10 dataset. The model is trained to reconstruct masked image patches in a self-supervised manner."
      ],
      "metadata": {
        "id": "3L6gAUvHFi0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup & Dependencies\n"
      ],
      "metadata": {
        "id": "JjyWLZn_H7ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install required libraries\n",
        "!pip install timm --quiet"
      ],
      "metadata": {
        "id": "Q5qvH8htIa74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import timm  # For pretrained ViT models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "TU_xBFvvKS8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "pvQv0u61Kf5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set random seed for consistency\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "akpJsg2FKzfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Dataset Preparation\n",
        "Here, we'll:\n",
        "\n",
        "Load CIFAR-10 data with normalization and augmentation\n",
        "\n",
        "Visualize some sample images\n",
        "\n",
        "Prepare for patching and masking in the next step"
      ],
      "metadata": {
        "id": "ET7915nDLbuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms: ToTensor and Normalize with CIFAR-10 stats\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),  # mean of CIFAR-10\n",
        "                         (0.2023, 0.1994, 0.2010))  # std of CIFAR-10\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "k0gy9XkBLgEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to unnormalize and show images\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))  # C x H x W --> H x W x C\n",
        "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "    std = np.array([0.2023, 0.1994, 0.2010])\n",
        "    img = std * img + mean  # unnormalize\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "# Show 8 sample images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for idx in range(8):\n",
        "    plt.subplot(1, 8, idx+1)\n",
        "    imshow(images[idx])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CIBMjGHVbmbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Patching & Masking\n",
        "In this section, we'll:\n",
        "\n",
        "Break the image into patches\n",
        "\n",
        "Randomly mask some patches\n",
        "\n",
        "Prepare them for ViT input"
      ],
      "metadata": {
        "id": "tr_M1shagc5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def patchify(imgs, patch_size=4):\n",
        "    \"\"\"\n",
        "    imgs: (B, C, H, W)\n",
        "    Returns: (B, N, patch_dim)\n",
        "    \"\"\"\n",
        "    B, C, H, W = imgs.shape\n",
        "    assert H % patch_size == 0 and W % patch_size == 0\n",
        "\n",
        "    h, w = H // patch_size, W // patch_size\n",
        "    patches = imgs.reshape(B, C, h, patch_size, w, patch_size)\n",
        "    patches = patches.permute(0, 2, 4, 3, 5, 1)  # [B, h, w, p, p, C]\n",
        "    patches = patches.reshape(B, h * w, patch_size * patch_size * C)  # [B, N, patch_dim]\n",
        "    return patches"
      ],
      "metadata": {
        "id": "8FUrLsZkgltq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_masking(patches, mask_ratio=0.75):\n",
        "    \"\"\"\n",
        "    Randomly mask a ratio of patches.\n",
        "    Returns:\n",
        "    - masked_patches: same shape as input, with some patches zeroed\n",
        "    - mask: binary mask of shape (B, N) where 1 = masked\n",
        "    \"\"\"\n",
        "    B, N, D = patches.shape\n",
        "    len_keep = int(N * (1 - mask_ratio))\n",
        "\n",
        "    noise = torch.rand(B, N, device=patches.device)  # Move to same device\n",
        "    ids_shuffle = noise.argsort(dim=1)\n",
        "    ids_restore = ids_shuffle.argsort(dim=1)\n",
        "\n",
        "    # keep the first part, mask the rest\n",
        "    ids_keep = ids_shuffle[:, :len_keep]\n",
        "\n",
        "    # create mask\n",
        "    mask = torch.ones(B, N, device=patches.device)\n",
        "    mask[:, :len_keep] = 0\n",
        "    mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "    # apply mask to patches\n",
        "    masked = torch.zeros_like(patches)\n",
        "    for b in range(B):\n",
        "        masked[b, ids_keep[b]] = patches[b, ids_keep[b]]\n",
        "\n",
        "    return masked, mask"
      ],
      "metadata": {
        "id": "5QHj0cC2hHob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch from train_loader\n",
        "sample_imgs, _ = next(iter(train_loader))\n",
        "sample_imgs = sample_imgs.to(device)\n",
        "\n",
        "# Convert to patches\n",
        "patches = patchify(sample_imgs)\n",
        "\n",
        "# Mask patches\n",
        "masked_patches, patch_mask = random_masking(patches, mask_ratio=0.75)\n",
        "\n",
        "print(\"Original patch shape:\", patches.shape)\n",
        "print(\"Masked patch shape:\", masked_patches.shape)\n",
        "print(\"Mask shape:\", patch_mask.shape)"
      ],
      "metadata": {
        "id": "XZZyvQP2hKsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Architecture\n",
        "Architecture Overview:\n",
        "\n",
        "Encoder: Pretrained or tiny ViT from timm (only processes visible patches).\n",
        "\n",
        "Decoder: A small MLP that reconstructs original patch pixels from latent vectors.\n",
        "\n",
        "Input: Masked patches\n",
        "\n",
        "Output: Reconstructed full patch sequence"
      ],
      "metadata": {
        "id": "q-lQk1OrjnYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "\n",
        "class PatchMindMAE(nn.Module):\n",
        "    def __init__(self, encoder_model='vit_tiny_patch16_224',\n",
        "                 patch_dim=48, embed_dim=192, decoder_dim=128, num_patches=64):\n",
        "        super(PatchMindMAE, self).__init__()\n",
        "\n",
        "        # ----- Encoder -----\n",
        "        self.encoder = timm.create_model(encoder_model, pretrained=True)\n",
        "        self.encoder.reset_classifier(0)\n",
        "        self.encoder.patch_embed.proj = nn.Linear(patch_dim, embed_dim)  # input patch_dim → ViT embed_dim\n",
        "\n",
        "        # ----- Decoder -----\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(embed_dim, decoder_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(decoder_dim, patch_dim)  # output same as original patch_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, masked_patches):\n",
        "        # masked_patches: [B, N, patch_dim]\n",
        "        x = self.encoder.patch_embed.proj(masked_patches)  # linear projection to embedding space\n",
        "        x = self.encoder.pos_drop(x + self.encoder.pos_embed[:, 1:x.size(1)+1, :])  # positional encoding\n",
        "        x = self.encoder.blocks(x)\n",
        "        x = self.encoder.norm(x)\n",
        "\n",
        "        # Decode to reconstruct patches\n",
        "        out = self.decoder(x)  # [B, N, patch_dim]\n",
        "        return out"
      ],
      "metadata": {
        "id": "u4D8SHJnj05k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PatchMindMAE(\n",
        "    encoder_model='vit_tiny_patch16_224',\n",
        "    patch_dim=48,      # 4x4 patch with 3 channels → 4*4*3 = 48\n",
        "    embed_dim=192,     # ViT tiny's default\n",
        "    decoder_dim=128,   # latent for MLP decoder\n",
        "    num_patches=64     # 32x32 / 4x4 = 64 patches\n",
        ").to(device)\n",
        "\n",
        "print(\"Model initialized. Total parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "id": "i5eFtc3EkAA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training Loop\n",
        "Here’s what we’ll do:\n",
        "\n",
        "Use MSE loss to compare reconstructed patches with original ones\n",
        "\n",
        "Only compute loss on masked patches\n",
        "\n",
        "Train for a few quick epochs\n",
        "\n",
        "Visualize the loss curve"
      ],
      "metadata": {
        "id": "Uxo9a9-bkWod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(original, reconstructed, mask):\n",
        "    \"\"\"\n",
        "    original, reconstructed: [B, N, patch_dim]\n",
        "    mask: [B, N] (1 = masked, 0 = visible)\n",
        "    \"\"\"\n",
        "    loss = ((original - reconstructed) ** 2).mean(dim=-1)  # [B, N]\n",
        "    loss = (loss * mask).sum() / mask.sum()  # only consider masked patches\n",
        "    return loss"
      ],
      "metadata": {
        "id": "EWX7eB-KkZ1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "num_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        imgs, _ = batch\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # Step 1: Patchify\n",
        "        patches = patchify(imgs)  # [B, 64, 48]\n",
        "\n",
        "        # Step 2: Mask patches\n",
        "        masked_patches, patch_mask = random_masking(patches, mask_ratio=0.75)\n",
        "\n",
        "        # Step 3: Forward pass\n",
        "        reconstructed = model(masked_patches)\n",
        "\n",
        "        # Step 4: Compute loss\n",
        "        loss = compute_loss(patches, reconstructed, patch_mask)\n",
        "\n",
        "        # Step 5: Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "sAb_nXMYkfy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(train_losses, marker='o')\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q-XZDs7IkiZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Evaluation — Visual Reconstruction\n",
        "We’ll:\n",
        "\n",
        "Take a few test images\n",
        "\n",
        "Mask them\n",
        "\n",
        "Let PatchMind reconstruct them\n",
        "\n",
        "Compare original vs reconstructed"
      ],
      "metadata": {
        "id": "h9g8-MUenmSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_and_visualize(model, dataloader, num_images=5, mask_ratio=0.75):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        imgs, _ = next(iter(dataloader))\n",
        "        imgs = imgs[:num_images].to(device)\n",
        "\n",
        "        # Patchify\n",
        "        patches = patchify(imgs)\n",
        "        masked_patches, patch_mask = random_masking(patches, mask_ratio)\n",
        "\n",
        "        # Reconstruct\n",
        "        output = model(masked_patches)\n",
        "\n",
        "        # Unpatchify for original and reconstructed\n",
        "        def unpatchify(patches, patch_size=4):\n",
        "            B, N, D = patches.shape\n",
        "            h = w = int(N**0.5)\n",
        "            C = D // (patch_size * patch_size)\n",
        "            patches = patches.reshape(B, h, w, patch_size, patch_size, C)\n",
        "            patches = patches.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
        "            imgs = patches.reshape(B, C, h * patch_size, w * patch_size)\n",
        "            return imgs\n",
        "\n",
        "        original_imgs = unpatchify(patches)\n",
        "        reconstructed_imgs = unpatchify(output)\n",
        "\n",
        "        # Visualize\n",
        "        for i in range(num_images):\n",
        "            plt.figure(figsize=(10, 3))\n",
        "            # Original\n",
        "            plt.subplot(1, 3, 1)\n",
        "            imshow(original_imgs[i].cpu())\n",
        "            plt.title('Original')\n",
        "            # Masked\n",
        "            plt.subplot(1, 3, 2)\n",
        "            imshow(unpatchify(masked_patches)[i].cpu())\n",
        "            plt.title('Masked')\n",
        "            # Reconstructed\n",
        "            plt.subplot(1, 3, 3)\n",
        "            imshow(reconstructed_imgs[i].cpu())\n",
        "            plt.title('Reconstructed')\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "final-cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
